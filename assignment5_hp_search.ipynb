{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T15:52:50.137448Z",
     "start_time": "2023-10-08T15:52:47.742955Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T15:52:50.147245Z",
     "start_time": "2023-10-08T15:52:50.141372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print(f\"Running on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T15:52:53.435540Z",
     "start_time": "2023-10-08T15:52:50.150369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T15:52:53.448058Z",
     "start_time": "2023-10-08T15:52:53.440867Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimeStopping:\n",
    "    def __init__(self, seconds):\n",
    "        self.seconds = seconds\n",
    "        self.start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def should_stop(self):\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        if elapsed_time > self.seconds:\n",
    "            print(f\"\\nStopping training: elapsed time {elapsed_time:.2f}s > limit {self.seconds}s\")\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T15:52:53.460466Z",
     "start_time": "2023-10-08T15:52:53.451399Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_units=512):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32*32*3, num_units)\n",
    "        self.bn = nn.BatchNorm1d(num_units)\n",
    "        self.fc2 = nn.Linear(num_units, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-08T15:52:47.508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Learning Rate: 0.001, Batch Size: 16, Num Units: 512, Loss: 1.7308477506637574\n",
      "Epoch: 2, Learning Rate: 0.001, Batch Size: 16, Num Units: 512, Loss: 1.5623120458030701\n",
      "Epoch: 3, Learning Rate: 0.001, Batch Size: 16, Num Units: 512, Loss: 1.4797582215118408\n",
      "Epoch: 4, Learning Rate: 0.001, Batch Size: 16, Num Units: 512, Loss: 1.414804999294281\n",
      "Epoch: 5, Learning Rate: 0.001, Batch Size: 16, Num Units: 512, Loss: 1.3598065354537965\n",
      "Epoch: 6, Learning Rate: 0.001, Batch Size: 16, Num Units: 512, Loss: 1.3152550214958192\n",
      "Epoch: 7, Learning Rate: 0.001, Batch Size: 16, Num Units: 512, Loss: 1.2790527084827423\n",
      "Epoch: 8, Learning Rate: 0.001, Batch Size: 16, Num Units: 512, Loss: 1.2335727715301514\n",
      "Epoch: 9, Learning Rate: 0.001, Batch Size: 16, Num Units: 512, Loss: 1.201054294614792\n",
      "Epoch: 10, Learning Rate: 0.001, Batch Size: 16, Num Units: 512, Loss: 1.1693300477027893\n",
      "Epoch: 1, Learning Rate: 0.001, Batch Size: 16, Num Units: 1024, Loss: 1.7905997957801818\n",
      "Epoch: 2, Learning Rate: 0.001, Batch Size: 16, Num Units: 1024, Loss: 1.583611333026886\n",
      "Epoch: 3, Learning Rate: 0.001, Batch Size: 16, Num Units: 1024, Loss: 1.4826828205871583\n",
      "Epoch: 4, Learning Rate: 0.001, Batch Size: 16, Num Units: 1024, Loss: 1.4065199402809143\n",
      "Epoch: 5, Learning Rate: 0.001, Batch Size: 16, Num Units: 1024, Loss: 1.355894774875641\n",
      "Epoch: 6, Learning Rate: 0.001, Batch Size: 16, Num Units: 1024, Loss: 1.2987467818832397\n",
      "Epoch: 7, Learning Rate: 0.001, Batch Size: 16, Num Units: 1024, Loss: 1.2597149966239929\n",
      "Epoch: 8, Learning Rate: 0.001, Batch Size: 16, Num Units: 1024, Loss: 1.2181239728546143\n",
      "Epoch: 9, Learning Rate: 0.001, Batch Size: 16, Num Units: 1024, Loss: 1.180321944885254\n",
      "Epoch: 10, Learning Rate: 0.001, Batch Size: 16, Num Units: 1024, Loss: 1.1357408931922912\n",
      "Epoch: 1, Learning Rate: 0.001, Batch Size: 16, Num Units: 1536, Loss: 1.8528195878601075\n",
      "Epoch: 2, Learning Rate: 0.001, Batch Size: 16, Num Units: 1536, Loss: 1.6023902004432677\n",
      "Epoch: 3, Learning Rate: 0.001, Batch Size: 16, Num Units: 1536, Loss: 1.488580499305725\n",
      "Epoch: 4, Learning Rate: 0.001, Batch Size: 16, Num Units: 1536, Loss: 1.4140703977203368\n",
      "Epoch: 5, Learning Rate: 0.001, Batch Size: 16, Num Units: 1536, Loss: 1.3524458697891235\n",
      "Epoch: 6, Learning Rate: 0.001, Batch Size: 16, Num Units: 1536, Loss: 1.3035084742546081\n",
      "Epoch: 7, Learning Rate: 0.001, Batch Size: 16, Num Units: 1536, Loss: 1.2561224113464355\n",
      "Epoch: 8, Learning Rate: 0.001, Batch Size: 16, Num Units: 1536, Loss: 1.2156294396781921\n",
      "Epoch: 9, Learning Rate: 0.001, Batch Size: 16, Num Units: 1536, Loss: 1.1716608301734925\n",
      "Epoch: 10, Learning Rate: 0.001, Batch Size: 16, Num Units: 1536, Loss: 1.1373521473407746\n",
      "Epoch: 1, Learning Rate: 0.001, Batch Size: 16, Num Units: 2048, Loss: 1.9159666538238525\n",
      "Epoch: 2, Learning Rate: 0.001, Batch Size: 16, Num Units: 2048, Loss: 1.6197295191955567\n",
      "Epoch: 3, Learning Rate: 0.001, Batch Size: 16, Num Units: 2048, Loss: 1.4903170400428771\n",
      "Epoch: 4, Learning Rate: 0.001, Batch Size: 16, Num Units: 2048, Loss: 1.4095362559509277\n",
      "Epoch: 5, Learning Rate: 0.001, Batch Size: 16, Num Units: 2048, Loss: 1.3568491081809997\n",
      "Epoch: 6, Learning Rate: 0.001, Batch Size: 16, Num Units: 2048, Loss: 1.2995463324832917\n",
      "Epoch: 7, Learning Rate: 0.001, Batch Size: 16, Num Units: 2048, Loss: 1.2592371850204467\n",
      "Epoch: 8, Learning Rate: 0.001, Batch Size: 16, Num Units: 2048, Loss: 1.2088551804542542\n",
      "Epoch: 9, Learning Rate: 0.001, Batch Size: 16, Num Units: 2048, Loss: 1.1704966358089448\n",
      "Epoch: 10, Learning Rate: 0.001, Batch Size: 16, Num Units: 2048, Loss: 1.13408267329216\n",
      "Epoch: 1, Learning Rate: 0.001, Batch Size: 32, Num Units: 512, Loss: 1.6779914956327744\n",
      "Epoch: 2, Learning Rate: 0.001, Batch Size: 32, Num Units: 512, Loss: 1.4930788095914165\n",
      "Epoch: 3, Learning Rate: 0.001, Batch Size: 32, Num Units: 512, Loss: 1.40074923693638\n",
      "Epoch: 4, Learning Rate: 0.001, Batch Size: 32, Num Units: 512, Loss: 1.3358043806719155\n",
      "Epoch: 5, Learning Rate: 0.001, Batch Size: 32, Num Units: 512, Loss: 1.2827320411582064\n",
      "Epoch: 6, Learning Rate: 0.001, Batch Size: 32, Num Units: 512, Loss: 1.2323970229139103\n",
      "Epoch: 7, Learning Rate: 0.001, Batch Size: 32, Num Units: 512, Loss: 1.1889389661818228\n",
      "Epoch: 8, Learning Rate: 0.001, Batch Size: 32, Num Units: 512, Loss: 1.1435147937260906\n",
      "Epoch: 9, Learning Rate: 0.001, Batch Size: 32, Num Units: 512, Loss: 1.099807868877894\n",
      "Epoch: 10, Learning Rate: 0.001, Batch Size: 32, Num Units: 512, Loss: 1.0601344033837394\n",
      "Epoch: 1, Learning Rate: 0.001, Batch Size: 32, Num Units: 1024, Loss: 1.7236192485726345\n",
      "Epoch: 2, Learning Rate: 0.001, Batch Size: 32, Num Units: 1024, Loss: 1.5186680547335327\n",
      "Epoch: 3, Learning Rate: 0.001, Batch Size: 32, Num Units: 1024, Loss: 1.4184884825960917\n"
     ]
    }
   ],
   "source": [
    "# Setup grid search parameters\n",
    "learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "batch_sizes = [16, 32]  # start with smaller batches\n",
    "num_units_values = list(range(512, 2049, 512))  # start with smaller model sizes\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Grid Search Loop\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        for nu in num_units_values:\n",
    "            \n",
    "            trainloader = DataLoader(trainset, batch_size=bs, shuffle=True)\n",
    "            model = MLP(num_units=nu).to(device)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            \n",
    "            for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "                running_loss = 0.0\n",
    "                for i, data in enumerate(trainloader, 0):\n",
    "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                    optimizer.zero_grad()  # zero the parameter gradients\n",
    "                    outputs = model(inputs)  # forward\n",
    "                    loss = criterion(outputs, labels)  # loss\n",
    "                    loss.backward()  # backward\n",
    "                    optimizer.step()  # optimize\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "                print(f\"Epoch: {epoch+1}, Learning Rate: {lr}, Batch Size: {bs}, Num Units: {nu}, Loss: {running_loss / len(trainloader)}\")\n",
    "            results.append((lr, bs, nu, running_loss / len(trainloader)))\n",
    "\n",
    "# Sorting results by loss to find the best combination\n",
    "best_combination = sorted(results, key=lambda x: x[3])[0]\n",
    "print(\"Best Combination: Learning Rate: {}, Batch Size: {}, Num Units: {}, Loss: {}\".format(*best_combination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
